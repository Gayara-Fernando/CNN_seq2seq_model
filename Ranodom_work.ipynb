{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d928af75-12ba-4df4-bd8d-b5205d8b92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like we have not stored the predicted values for the test features - the extracted 32 featrues for each subwindow. We need this for the stage two model. Let's not store it here, but we will still need to get these, and let's have a look at this here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9cddd5-3960-414c-8fd9-ab62dcfcdc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:20:44.501598: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 12:20:44.532728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe8ff8b-f5a8-412e-91b7-5cb19bb0d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is the trained model?\n",
    "\n",
    "# consider the first non-overlapping model we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114efb20-0967-490d-967e-03a30de35303",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlapping_model = tf.keras.models.load_model(\"models/CNN_seq2seq_non_overlapping.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936a70f8-5ad3-42a7-8326-fd9e0166fc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 13, None, None, 3)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 13, 32)               71808     ['input_1[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 64),                 24832     ['time_distributed[0][0]']    \n",
      "                              (None, 64),                                                         \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 7, 64)                0         ['lstm[0][0]']                \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 7, 64)                33024     ['repeat_vector[0][0]',       \n",
      "                                                                     'lstm[0][1]',                \n",
      "                                                                     'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 7, 32)                2080      ['lstm_1[0][0]']              \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 131744 (514.62 KB)\n",
      "Trainable params: 103104 (402.75 KB)\n",
      "Non-trainable params: 28640 (111.88 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "non_overlapping_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789f3352-49fe-4f89-aa90-7b5a4330e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, where is our test input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa689a2-084d-4cd8-8043-a745b3ec2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features\n",
    "input_features_loc = 'data/test_input_sub_images'\n",
    "input_contents = os.listdir(input_features_loc)\n",
    "input_contents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d2c051a-0b59-4eea-b3cd-f70215bb81b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data_blk_0103.npy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972c85ad-5460-4e73-b038-d0abfd81c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just od this here for a single block, and see how these look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd59fb0-02dd-40bc-9d0b-1e9bfa6a8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_0103_features = np.load(os.path.join(input_features_loc, input_contents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab0cb277-aa90-43d9-8bf4-4406719527aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(910, 13, 30, 30, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_0103_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0055b74-75d2-4f5a-9f13-3cdd1e34cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52638fe7-fc4c-4191-8aea-866cfd73008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "blk_0103_predicted_features = non_overlapping_model.predict(block_0103_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95f8a4eb-7f13-4649-ac8c-ea1a4af59f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(910, 7, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk_0103_predicted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b8e9cd0-b58b-4e51-bc09-963b1179a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These predicted features seem to be a little high than the features that we have had before for training of the BLAR process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401bec62-5f0e-48e8-999d-352c61d3c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine this data for a bit before proceeding - I mean at this point, we can blindly go ahead and run the BLAR model, but let's try and figure out what limitations we have in our data - then we might be able to even fit better models in the first stage to get better final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f9c24e-f4a1-4657-a225-7f9a27400e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, notice here, we only have our test data, but for the BLAR model, we also need the 32 features for each sub-image in the sequence - maybe we extract that from the previous model like how it was done ? (May need to check the preprocessing script for comps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_cpu_env)",
   "language": "python",
   "name": "tfp_cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
