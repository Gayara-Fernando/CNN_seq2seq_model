{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1461d38b-cf33-42ff-808b-cc345321e8a2",
   "metadata": {},
   "source": [
    "##### Stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f783441-092e-4a11-a42a-8434a03cc103",
   "metadata": {},
   "source": [
    "We are back at stage 1. Stage 2 will use the input from stage 1 as discussed before. We just have a new model architecture, which will use the sub-image sequence as the input, and the targets will be the extracted features sequences, which will be used as input into the Stage 2 Bayesian latent AR model. Also for the current work we consider all sub-windows to be non-overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489f5dd-f89f-47a3-b26f-bfe4579842ba",
   "metadata": {},
   "source": [
    "In notebook 0, we have the model architecture for the said input and output data. We will go from preprocessing to training to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1312ed4-4e78-4a47-b5f9-520344191ab2",
   "metadata": {},
   "source": [
    "There are 2 notebook 1s. The first Notebook 1 does the preprocessing for the inputs of the model, and the second does the preprocessing for the targets. For the preprocessing the targets, we will use the last 7 image sequences for each block (sub-image sequences to be exact), and extract the features. These will be the targets we use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36948ed-c975-47d8-8c52-ba938acf5dc0",
   "metadata": {},
   "source": [
    "We will extend this work to overlapping subwindows of images, We will need to work on preprocessing, and training (and might need to use a generator here). The only downside in this would be we will have a lot of subwindows of images, and this increased size will force the stage 2 Bayesian latent AR model to take a lot of time for execute and produce results. This work is in the notebooks 4, 5 and 6. Data are inside the data folder in the \"Overlapping_data\" subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0826a0-7ee1-499b-b800-1ea1c4fd9964",
   "metadata": {},
   "source": [
    "Also, the activation we are using in the last layer is currently a \"Relu\", let's maybe changed this and see if there are any improvements at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee02ea7-705f-4f1c-8cd6-2e382e0c8275",
   "metadata": {},
   "source": [
    "We will do notebooks 7,8 for training and inference for non-overlapping sub windows, without Relu as the activation function in the last layer. And then 9,10 for tarining and inference for overlapping sub-windows without Relu activation (We will use the linear activation function in the last layer of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ac603-0503-45d7-85d5-5688f842602a",
   "metadata": {},
   "source": [
    "##### Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e5d1b-8bdf-4dd1-90b5-d979b4f911d7",
   "metadata": {},
   "source": [
    "###### Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6780a-6c52-4554-ac4f-91665b9b415b",
   "metadata": {},
   "source": [
    "We will do a base model for comparison. (We will need to do two cases for overlapping and non-overlapping, we will start with the regular non-overlapping case). We will first need to process the data for the targets (as these now will be the densities). The inputs will be the same, we will use the subwindows of images instead of the extrcated features. The inputs for the model therefore will have the shape (3640, 13, 30, 30, 3) and (3640, 7) will be the shape of the targets. We will first preprocess the target data, and then get to model training. Refer notebook 11 for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1da481-9a29-4e71-815f-4eb9dd646f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nrdstor_tfp_for_TN)",
   "language": "python",
   "name": "nrdstor_tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
