{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1461d38b-cf33-42ff-808b-cc345321e8a2",
   "metadata": {},
   "source": [
    "##### Stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f783441-092e-4a11-a42a-8434a03cc103",
   "metadata": {},
   "source": [
    "We are back at stage 1. Stage 2 will use the input from stage 1 as discussed before. We just have a new model architecture, which will use the sub-image sequence as the input, and the targets will be the extracted features sequences, which will be used as input into the Stage 2 Bayesian latent AR model. Also for the current work we consider all sub-windows to be non-overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489f5dd-f89f-47a3-b26f-bfe4579842ba",
   "metadata": {},
   "source": [
    "In notebook 0, we have the model architecture for the said input and output data. We will go from preprocessing to training to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1312ed4-4e78-4a47-b5f9-520344191ab2",
   "metadata": {},
   "source": [
    "There are 2 notebook 1s. The first Notebook 1 does the preprocessing for the inputs of the model, and the second does the preprocessing for the targets. For the preprocessing the targets, we will use the last 7 image sequences for each block (sub-image sequences to be exact), and extract the features. These will be the targets we use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36948ed-c975-47d8-8c52-ba938acf5dc0",
   "metadata": {},
   "source": [
    "We will extend this work to overlapping subwindows of images, We will need to work on preprocessing, and training (and might need to use a generator here). The only downside in this would be we will have a lot of subwindows of images, and this increased size will force the stage 2 Bayesian latent AR model to take a lot of time for execute and produce results. This work is in the notebooks 4, 5 and 6. Data are inside the data folder in the \"Overlapping_data\" subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0826a0-7ee1-499b-b800-1ea1c4fd9964",
   "metadata": {},
   "source": [
    "Also, the activation we are using in the last layer is currently a \"Relu\", let's maybe changed this and see if there are any improvements at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee02ea7-705f-4f1c-8cd6-2e382e0c8275",
   "metadata": {},
   "source": [
    "We will do notebooks 7,8 for training and inference for non-overlapping sub windows, without Relu as the activation function in the last layer. And then 9,10 for tarining and inference for overlapping sub-windows without Relu activation (We will use the linear activation function in the last layer of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ac603-0503-45d7-85d5-5688f842602a",
   "metadata": {},
   "source": [
    "##### Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e5d1b-8bdf-4dd1-90b5-d979b4f911d7",
   "metadata": {},
   "source": [
    "###### Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6780a-6c52-4554-ac4f-91665b9b415b",
   "metadata": {},
   "source": [
    "We will do a base model for comparison. (We will need to do two cases for overlapping and non-overlapping, we will start with the regular non-overlapping case). We will first need to process the data for the targets (as these now will be the densities). The inputs will be the same, we will use the subwindows of images instead of the extrcated features. The inputs for the model therefore will have the shape (3640, 13, 30, 30, 3) and (3640, 7) will be the shape of the targets. We will first preprocess the target data, and then get to model training. Refer notebook 11 for preprocessing. Notebook 12 will have the train script (take a look at this model architecture, this seemms like a better version of the CNN-LSTM model fillted - so maybe report this instead of the results shared with Dr. G?), and 13 is for inference. The metrics are in the inference script, the predicted and true values are stored in the \"data/Stage2_base_model/Non_overlapping/predicted_and_true_densities/\" folder. Also, to note here that all work for stage 2 base model is store in the folder \"data/Stage2_base_model/\". All work for non-overlapping subwindows in the folder \"Non_overlapping\", and all work for overlappinf subwindows in folder \"Overlapping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6eea3-2ab4-4bae-80b5-d0be2314c509",
   "metadata": {},
   "source": [
    "For the overlapping subwindows for base model, the preprocessing script is in notebook 14, training script in 15, and the inference in 16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4850a1-d3c8-4168-b3b9-0ca14648105d",
   "metadata": {},
   "source": [
    "###### Latent AR process model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adb1e7-3045-4b39-b67a-4eadaa038097",
   "metadata": {},
   "source": [
    "Let's do this work in a separate location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15836c5e-0142-4d4a-be8d-156b9c8a994b",
   "metadata": {},
   "source": [
    "Also, we have not stored the predictions from the model for the generated test sequences here in this folder, let's do that also in the new location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47307af5-8e34-4593-8671-d12172ccd869",
   "metadata": {},
   "source": [
    "We may also need to train newer models apart from the once we have already done, vary the recurrent dropouts and other parameters to improve the predictions of the sequences for the test data as the BLAR process model need the best predictions we can get from the stage 1 modeling phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nrdstor_tfp_for_TN)",
   "language": "python",
   "name": "nrdstor_tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
